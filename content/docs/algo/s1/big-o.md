---
title: "Асимптотическая сложность (статья-пример)"
weight: 1
---

{{< katex />}}

# Асимптотическая сложность (Big O)

Перед тем как сравнивать алгоритмы, нужно договориться, как мы измеряем их «скорость» и расход памяти. Для этого используют **асимптотическую сложность** — поведение времени работы или памяти при росте размера входных данных $n$.

## Зачем это нужно

- Один и тот же алгоритм на разных машинах даёт разное время в секундах, но **порядок роста** (линейный, квадратичный и т.д.) не зависит от железа.
- На больших $n$ именно порядок роста определяет, уложится ли решение в ограничения по времени и памяти.

Поэтому в олимпиадах и собеседованиях смотрят на **сложность по времени и по памяти**, а не на константы и секунды.

## Обозначение O (Big O)

**Big O** — это верхняя оценка «в худшем случае». Говорят: «алгоритм работает за $O(n^2)$», если при росте $n$ время растёт не быстрее, чем некоторая константа, умноженная на $n^2$.

Формально: $T(n) = O(g(n))$, если существуют константы $c > 0$ и $n_0$ такие, что для всех $n \geq n_0$ выполняется $T(n) \leq c \cdot g(n)$.

$$
T(n) = O(g(n)) \Leftrightarrow \exists c,\, n_0 : \forall n \geq n_0 \quad T(n) \leq c \cdot g(n)
$$

На практике мы отбрасываем младшие слагаемые и константы: $O(5n^2 + 3n + 1)$ записывают как $O(n^2)$.

## Основные классы сложности

| Обозначение | Название        | Пример при $n = 10^6$ (порядок) |
|-------------|-----------------|--------------------------------------|
| $O(1)$  | Константная     | Мгновенно                            |
| $O(\log n)$ | Логарифмическая | Очень быстро                    |
| $O(n)$  | Линейная        | Секунды                              |
| $O(n \log n)$ | Линейно-логарифмическая | Секунды        |
| $O(n^2)$ | Квадратичная   | Часы / не пройдёт по времени         |
| $O(2^n)$ | Экспоненциальная | Неприменимо на больших $n$    |

Чем выше по этой таблице, тем лучше (при прочих равных).

## Примеры

**$O(1)$** — доступ к элементу массива по индексу, добавление в конец динамического массива (амортизированно).

**$O(\log n)$** — поиск в сбалансированном двоичном дереве поиска, бинарный поиск по отсортированному массиву.

**$O(n)$** — линейный поиск в массиве, один проход по списку, обход дерева в глубину.

**$O(n \log n)$** — эффективные сортировки сравнением: merge sort, heapsort, быстрая сортировка в среднем.

**$O(n^2)$** — пузырьковая сортировка, два вложенных цикла по массиву.

**$O(2^n)$** — наивный перебор всех подмножеств множества из $n$ элементов.

## Omega и Theta

- **$O$ (Big O)** — верхняя оценка: «не хуже, чем …».
- **$\Omega$ (Omega)** — нижняя оценка: «как минимум как …».
- **$\Theta$ (Theta)** — точный порядок: и $O$, и $\Omega$ совпадают. Например, «сортировка сравнением в среднем $\Theta(n \log n)$».

В повседневной речи чаще говорят «работает за $O(\ldots)$», имея в виду порядок роста в худшем или среднем случае.

## Что запомнить

1. Оцениваем время и память как функцию от $n$ (размер входа).
2. Константы и младшие слагаемые отбрасываем — важен только порядок роста.
3. Big O — верхняя оценка; для нижней используют $\Omega$, для точного порядка — $\Theta$.
4. На больших $n$ доминирует старшая степень или экспонента — поэтому $O(n^2)$ «хуже», чем $O(n \log n)$.

С этой базой можно переходить к разбору конкретных алгоритмов и структур данных и оценивать их применимость по ограничениям задачи.
